# Turtle Talk — Cursor Rules

## Project Overview
Children's AI voice-chat app. A child speaks to Shelly, a friendly sea turtle, who listens, responds with voice, and suggests personal growth missions (e.g. being brave, making friends, staying calm).

## Tech Stack
- **Next.js 16** (App Router), **React 19**, **TypeScript 5**
- **Tailwind CSS v4** — use `@import "tailwindcss"` not `@tailwind` directives
- **Radix UI Themes** (`@radix-ui/themes`) — available but used sparingly
- **LangChain** (`@langchain/anthropic`, `@langchain/openai`) — for AI chat
- **Jest 30** + `babel-jest` + `jest-environment-jsdom` — test runner

## Environment Variables (`.env.local`)
```
OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...

# All model/voice settings — defaults are set in lib/speech/config.ts
SPEECH_CHAT_PROVIDER=anthropic          # "anthropic" | "openai"
SPEECH_ANTHROPIC_MODEL=claude-haiku-4-5-20251001
SPEECH_OPENAI_MODEL=gpt-4o-mini
SPEECH_CHAT_MAX_TOKENS=256
SPEECH_STT_MODEL=gpt-4o-mini-transcribe
SPEECH_TTS_MODEL=gpt-4o-mini-tts
SPEECH_TTS_VOICE=coral                  # alloy|ash|ballad|coral|echo|fable|nova|sage|shimmer|verse
SPEECH_TTS_INSTRUCTIONS=...             # voice personality instructions for gpt-4o-mini-tts
```

## Directory Structure
```
app/
  page.tsx                        ← Home page (animated ocean scene + nav)
  layout.tsx                      ← Root layout: Scene bg + Radix Theme wrapper
  globals.css                     ← Tailwind v4, keyframe animations (day/night, turtle bob, moods)
  talk/page.tsx                   ← Conversation page (mic permission → VAD → talk loop)
  missions/page.tsx               ← Missions page (Active / Completed tabs)
  api/talk/route.ts               ← POST /api/talk — audio in, {userText, responseText, audio, mood, mission} out
  components/
    Scene.tsx                     ← Animated ocean background (fixed, z-0)
    VersionBadge.tsx              ← Small version badge
    talk/
      TurtleCharacter.tsx         ← SVG turtle with 7 mood states + CSS animation classes
      MuteButton.tsx              ← 56px+ green/red mic toggle
      EndButton.tsx               ← 56px+ red end button
      MicPermission.tsx           ← Full-screen permission request card
  hooks/
    useMicPermission.ts           ← checking | prompt | granted | denied
    useSpeechConversation.ts      ← VAD state machine + API calls + audio playback
    useMissions.ts                ← localStorage-backed mission CRUD

lib/speech/
  config.ts                       ← Central model/voice config with env var overrides
  types.ts                        ← All shared types (see below)
  errors.ts                       ← SpeechServiceError, GuardrailBlockedError
  SpeechService.ts                ← Orchestrator: STT → guardrails → chat → guardrails → TTS
  index.ts                        ← Barrel export
  providers/
    chat.ts                       ← AnthropicChatProvider (haiku-4-5) / OpenAIChatProvider (gpt-4o-mini)
    stt.ts                        ← OpenAISTTProvider (whisper-1)
    tts.ts                        ← OpenAITTSProvider (nova, mp3)
  guardrails/
    types.ts                      ← GuardrailAgent interface, GuardrailResult
    ChildSafeGuardrail.ts         ← Regex/keyword fast check (always-on)
    LLMGuardrail.ts               ← Stub for LLM-based check

__tests__/
  components/talk/                ← TurtleCharacter, MuteButton, EndButton, MicPermission
  services/                       ← SpeechService, guardrails
```

## Key Types (`lib/speech/types.ts`)
```ts
type TurtleMood = 'idle' | 'listening' | 'talking' | 'happy' | 'sad' | 'confused' | 'surprised'
type MissionTheme = 'brave' | 'kind' | 'calm' | 'confident' | 'creative' | 'social' | 'curious'

interface Mission {
  id: string; title: string; description: string; theme: MissionTheme;
  status: 'active' | 'completed'; createdAt: string; completedAt?: string;
}
interface MissionSuggestion { title: string; description: string; theme: MissionTheme; }
interface Message { role: 'user' | 'assistant'; content: string; }
interface ProcessResult { userText; responseText; responseAudio: ArrayBuffer; mood; mission?: MissionSuggestion; }
```

## API Contract
**POST `/api/talk`** — `multipart/form-data`
- `audio`: Blob (webm)
- `messages`: JSON string of `Message[]`

Response JSON:
```json
{ "userText": "...", "responseText": "...", "responseAudioBase64": "...", "mood": "happy", "mission": null | { "title": "...", "description": "...", "theme": "brave" } }
```

## Providers (`lib/speech/providers/`)
All providers implement simple interfaces from `lib/speech/types.ts`. Swap via `SPEECH_CHAT_PROVIDER` env var or by passing a different provider to `SpeechService`.

### STT — `OpenAISTTProvider`
- Model: `gpt-4o-mini-transcribe` (override: `SPEECH_STT_MODEL`)
- Input: `Blob` (audio/webm from MediaRecorder)
- Output: transcribed `string`
- Interface: `STTProvider { transcribe(audio: Blob): Promise<string> }`

### TTS — `OpenAITTSProvider`
- Model: `gpt-4o-mini-tts`, voice: `coral`, format: `mp3` (overrides: `SPEECH_TTS_MODEL`, `SPEECH_TTS_VOICE`, `SPEECH_TTS_INSTRUCTIONS`)
- Passes a `instructions` string to control Shelly's voice personality and pacing
- Input: `string`
- Output: `ArrayBuffer` (decoded by Web Audio API on the client)
- Interface: `TTSProvider { synthesize(text: string): Promise<ArrayBuffer> }`

### Chat — `AnthropicChatProvider` / `OpenAIChatProvider`
- Anthropic: `claude-haiku-4-5-20251001`, maxTokens: 256 (overrides: `SPEECH_ANTHROPIC_MODEL`, `SPEECH_CHAT_MAX_TOKENS`)
- OpenAI: `gpt-4o-mini`, maxTokens: 256 (overrides: `SPEECH_OPENAI_MODEL`, `SPEECH_CHAT_MAX_TOKENS`)
- Both extend `BaseChatProvider` which handles message history formatting via LangChain
- Interface: `ChatProvider { chat(input: string, ctx: ConversationContext): Promise<ChatResponse> }`
- History is passed as alternating `HumanMessage` / `SystemMessage` (note: assistant turns use `SystemMessage` — intentional LangChain quirk)
- Optional `ctx.childName` appends a personalisation line to the system prompt
- Factory: `createChatProvider('anthropic' | 'openai')` in `providers/chat.ts`

## Shelly System Prompt (`lib/speech/providers/chat.ts` → `SHELLY_SYSTEM_PROMPT`)
The full prompt enforces:
1. **Persona** — kind, curious sea turtle for ages 4–10; 2–3 sentence max; simple words
2. **Strict JSON output** — every response must be valid JSON: `{ "text": "...", "mood": "..." }`
3. **Mood selection rules**:
   - `happy` — good news, fun facts, joy
   - `sad` — empathy, unfortunate topics
   - `confused` — unclear topic, needs clarification
   - `surprised` — exciting or unexpected
   - `talking` — default
4. **Mission generation** — optional `"mission"` field added when Shelly identifies a personal challenge:
   ```json
   { "text": "...", "mood": "happy", "mission": { "title": "...", "description": "...", "theme": "social" } }
   ```
   - Only suggest missions occasionally, not every turn
   - Theme must be one of: `brave | kind | calm | confident | creative | social | curious`
   - Description must be friendly and actionable for a young child

**Parsing** (`parseChatResponse`): JSON.parse on the raw LLM string; falls back to `{ text: raw, mood: 'happy' }` if parse fails. Mission validated by `parseMission` — invalid theme defaults to `'curious'`.

## Guardrails (`lib/speech/guardrails/`)
Guardrails implement `GuardrailAgent` and run in two phases: **input** (after STT, before chat) and **output** (after chat, before TTS). Multiple guardrails chain in order.

### Interface (`guardrails/types.ts`)
```ts
interface GuardrailResult { safe: boolean; reason?: string; sanitized?: string; }
interface GuardrailAgent {
  name: string;
  checkInput(text: string): Promise<GuardrailResult>;
  checkOutput(text: string): Promise<GuardrailResult>;
}
```

### `ChildSafeGuardrail` (always active)
- **Input check**: regex scan against 5 categories — violence, adult content, profanity, self-harm, substances. Returns `safe: false` on first match.
- **Output check**: same regex scan + truncates output to 500 chars (returns `sanitized` if truncated).
- Blocked input → `SpeechService` returns fallback response instead of calling chat.
- Blocked output → fallback response replaces LLM output.

### `LLMGuardrail` (stub — not wired in by default)
- Located at `guardrails/LLMGuardrail.ts`
- Passes everything as safe currently (TODO stubs)
- Intended for LLM-based deep content classification
- To activate: instantiate and pass to `SpeechService` config alongside `ChildSafeGuardrail`

### Adding a new guardrail
```ts
// 1. Implement GuardrailAgent
export class MyGuardrail implements GuardrailAgent {
  readonly name = 'MyGuardrail';
  async checkInput(text: string): Promise<GuardrailResult> { ... }
  async checkOutput(text: string): Promise<GuardrailResult> { ... }
}

// 2. Pass to SpeechService in app/api/talk/route.ts
const service = new SpeechService({ stt, tts, chat, guardrails: [new ChildSafeGuardrail(), new MyGuardrail()] });
// or dynamically:
service.addGuardrail(new MyGuardrail());
```

## Processing Pipeline (`lib/speech/SpeechService.ts`)
```
audio (Blob)
  │
  ▼ 1. STT          OpenAISTTProvider.transcribe()  → userText (string)
  │
  ▼ 2. Input        ChildSafeGuardrail.checkInput()
     Guardrails     [+ any additional guardrails]
     ↳ blocked?  →  fallback response (skip chat + output guardrails)
  │
  ▼ 3. Chat         ChatProvider.chat(userText, context)  → { text, mood, mission? }
  │
  ▼ 4. Output       ChildSafeGuardrail.checkOutput()
     Guardrails     [+ any additional guardrails]
     ↳ blocked?  →  fallback response
     ↳ sanitized? → use sanitized text
  │
  ▼ 5. TTS          OpenAITTSProvider.synthesize(safeText)  → ArrayBuffer
  │
  ▼ ProcessResult { userText, responseText, responseAudio, mood, mission? }
```
Fallback response: `"Oh my! Let's talk about something else. What's your favourite animal?"` (mood: `confused`)

## LangChain Usage
- **No LangChain agents or tools are defined yet** — only the chat model abstraction is used
- `BaseChatModel.invoke(messages[])` is the only LangChain call
- Future agents/tools (e.g. emotion analysis, parental controls, mission generation agent) would extend this layer
- Add tools by wrapping `ChatAnthropic`/`ChatOpenAI` with `createReactAgent` from `langchain/agents`

## VAD State Machine (`useSpeechConversation`)
```
idle → startListening() → listening → (voice detected) → recording → (silence) → processing → speaking → listening
                                                                                   ↓ onMission?.(mission)
```
- `onEnd` callback → navigates to `/missions`
- `onMission` callback → saves mission via `useMissions.addMission`

## Missions Flow
1. Shelly's chat response optionally includes a `mission` object
2. API route passes it through to the client
3. `useSpeechConversation` fires `onMission` when present
4. `useMissions` hook stores missions in `localStorage` under key `turtle-talk-missions`
5. `/missions` page shows Active / Completed tabs; "✓ Done!" marks complete

## Styling Conventions
- All pages: `position: relative; z-index: 10; min-height: 100vh` to render above the Scene background
- Inline styles are used throughout (consistent with existing codebase) — prefer inline styles over new Tailwind classes for new components
- Color palette: white text, `rgba(255,255,255,0.x)` for tinted surfaces, ocean blues
- Drop shadows on text: `textShadow: '0 2px 8px rgba(0,0,0,0.4)'`
- Backdrop blur on cards/buttons: `backdropFilter: 'blur(8px)'`
- Buttons: min 56px touch target for interactive controls in the talk UI
- Tailwind v4 is available for utility classes (e.g. `className="relative z-10 ..."`

## Testing Notes
- SVG `className` in jsdom returns `SVGAnimatedString` — use `element.getAttribute('class')` not `.className`
- Path alias `@/*` resolves to project root (not `src/`)
- `jest.config.ts` uses `setupFilesAfterEnv` (not `setupFilesAfterFramework`)
- Run tests: `npm test`

## Commands
```bash
npm run dev      # dev server on :3000
npm run build    # production build
npm test         # Jest (63 tests)
npx tsc --noEmit # type-check only
```
